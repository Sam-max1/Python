{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning in Python for Data Science\n",
    "\n",
    "Data cleaning is a crucial step in data science. Here's how to use Python for various data cleaning tasks:\n",
    "\n",
    "### 1. Basic Data Cleaning with Pandas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample dataset with common issues\n",
    "data = {\n",
    "    'name': ['John', 'Alice', np.nan, 'Bob', 'John'],\n",
    "    'age': [28, -24, 32, 1000, 28],\n",
    "    'salary': ['50000', '45,000', 'unknown', '70000', '50000'],\n",
    "    'date': ['2023-01-01', '2023-13-01', '2023-01-15', '2023-01-32', '2023-01-01']\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 1. Handle missing values\n",
    "df['name'] = df['name'].fillna('Unknown')  # Fill NaN with 'Unknown'\n",
    "\n",
    "# 2. Remove duplicates\n",
    "df = df.drop_duplicates()  # Remove duplicate rows\n",
    "\n",
    "# 3. Fix data types\n",
    "df['salary'] = df['salary'].replace('unknown', np.nan)  # Replace 'unknown' with NaN\n",
    "df['salary'] = df['salary'].str.replace(',', '')  # Remove commas\n",
    "df['salary'] = pd.to_numeric(df['salary'], errors='coerce')  # Convert to numeric\n",
    "\n",
    "# 4. Handle outliers using IQR method\n",
    "Q1 = df['age'].quantile(0.25)\n",
    "Q3 = df['age'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "df = df[~((df['age'] < (Q1 - 1.5 * IQR)) | (df['age'] > (Q3 + 1.5 * IQR)))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 2. Advanced Data Cleaning with Regular Expressions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import re\n",
    "\n",
    "# Clean text data\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        # Remove special characters\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "        # Remove extra whitespace\n",
    "        text = ' '.join(text.split())\n",
    "        return text\n",
    "    return text\n",
    "\n",
    "# Apply text cleaning to a column\n",
    "df['name'] = df['name'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 3. Date Cleaning and Validation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Function to validate and clean dates\n",
    "def clean_date(date_str):\n",
    "    try:\n",
    "        # Try to parse the date\n",
    "        return pd.to_datetime(date_str)\n",
    "    except:\n",
    "        return pd.NaT  # Return Not-a-Time for invalid dates\n",
    "\n",
    "# Clean date column\n",
    "df['date'] = df['date'].apply(clean_date)\n",
    "\n",
    "# Remove rows with invalid dates\n",
    "df = df.dropna(subset=['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 4. Handling Missing Values with Different Strategies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for advanced imputation\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# 1. Simple imputation strategies\n",
    "df['age'] = df['age'].fillna(df['age'].mean())  # Fill with mean\n",
    "df['salary'] = df['salary'].fillna(df['salary'].median())  # Fill with median\n",
    "df['name'] = df['name'].fillna(df['name'].mode()[0])  # Fill with mode\n",
    "\n",
    "# 2. Advanced imputation using SimpleImputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df[['age', 'salary']] = imputer.fit_transform(df[['age', 'salary']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 5. Data Validation and Consistency Checks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define validation functions\n",
    "def validate_age(age):\n",
    "    return 0 <= age <= 120  # Age between 0 and 120\n",
    "\n",
    "def validate_salary(salary):\n",
    "    return 0 <= salary <= 1000000  # Salary between 0 and 1M\n",
    "\n",
    "# Apply validation and filter data\n",
    "mask = (\n",
    "    df['age'].apply(validate_age) &\n",
    "    df['salary'].apply(validate_salary)\n",
    ")\n",
    "df = df[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 6. Creating Clean Features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new features from clean data\n",
    "df['age_group'] = pd.cut(\n",
    "    df['age'],\n",
    "    bins=[0, 25, 35, 50, 120],\n",
    "    labels=['Young', 'Adult', 'Middle-aged', 'Senior']\n",
    ")\n",
    "\n",
    "df['salary_category'] = pd.qcut(\n",
    "    df['salary'],\n",
    "    q=3,\n",
    "    labels=['Low', 'Medium', 'High']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Key Points to Remember:\n",
    "\n",
    "1. **Initial Data Assessment**\n",
    "- Always examine your data first using:\n",
    "  ```python\n",
    "  df.info()  # Data types and missing values\n",
    "  df.describe()  # Statistical summary\n",
    "  df.isnull().sum()  # Count missing values\n",
    "  ```\n",
    "\n",
    "2. **Data Cleaning Steps**\n",
    "- Handle missing values\n",
    "- Remove duplicates\n",
    "- Fix data types\n",
    "- Handle outliers\n",
    "- Validate data\n",
    "- Create consistent formats\n",
    "\n",
    "3. **Best Practices**\n",
    "- Keep the original data\n",
    "- Document all cleaning steps\n",
    "- Validate results after each step\n",
    "- Use automated cleaning pipelines for repeatability\n",
    "\n",
    "4. **Common Data Issues**\n",
    "- Missing values\n",
    "- Duplicates\n",
    "- Inconsistent formats\n",
    "- Outliers\n",
    "- Invalid data\n",
    "- Wrong data types\n",
    "\n",
    "5. **Saving Clean Data**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned dataset\n",
    "df.to_csv('cleaned_data.csv', index=False)\n",
    "# For Excel\n",
    "df.to_excel('cleaned_data.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This process creates a clean, consistent dataset ready for analysis and modeling in your data science projects.\n",
    "\n",
    "Similar code found with 1 license type"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
